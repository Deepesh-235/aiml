<!DOCTYPE html>
<html>
<head>
  <title>Python BFS Code</title>
  <style>
    body {
      font-family: monospace;
      background-color: #f0f0f0;
      padding: 20px;
    }
    pre {
      background-color: #fff;
      padding: 15px;
      border: 1px solid #ccc;
      overflow-x: auto;
    }
  </style>
</head>
<body>

  <h2>Breadth First Search (Python Code)</h2>

  <pre>
from collections import deque

class Graph:
    def __init__(self):
        # Dictionary to store the graph (adjacency list)
        self.graph = {}

    def add_edge(self, u, v):
        # Add an edge from u to v
        if u not in self.graph:
            self.graph[u] = []
        self.graph[u].append(v)

    def bfs(self, start):
        visited = set()         # To track visited nodes
        queue = deque([start])  # Use deque for efficient queue operations
        visited.add(start)

        while queue:
            node = queue.popleft()
            print(node, end=" ")

            for neighbor in self.graph.get(node, []):
                if neighbor not in visited:
                    queue.append(neighbor)
                    visited.add(neighbor)

# ---------- Test the code ----------

g = Graph()
g.add_edge(0, 1)
g.add_edge(0, 2)
g.add_edge(1, 2)
g.add_edge(2, 0)
g.add_edge(2, 3)
g.add_edge(3, 3)

print("Breadth First Traversal (starting from vertex 2):")
g.bfs(2)
  </pre>

<h2>dfs</h2>

  <pre>
    class Graph:
    def __init__(self):
        # Dictionary to store the graph as an adjacency list
        self.graph = {}

    def add_edge(self, u, v):
        # Add an edge from u to v
        if u not in self.graph:
            self.graph[u] = []
        self.graph[u].append(v)

    def dfs_util(self, node, visited):
        visited.add(node)
        print(node, end=" ")

        for neighbor in self.graph.get(node, []):
            if neighbor not in visited:
                self.dfs_util(neighbor, visited)

    def dfs(self, start):
        visited = set()
        self.dfs_util(start, visited)

# ---------- Test the code ----------

g = Graph()
g.add_edge(0, 1)
g.add_edge(0, 2)
g.add_edge(1, 2)
g.add_edge(2, 0)
g.add_edge(2, 3)
g.add_edge(3, 3)

print("Depth First Traversal (starting from vertex 2):")
g.dfs(2)

  </pre>


<h2>a star</h2>

  <pre>
    class Graph:
    def __init__(self, graph):
        self.graph = graph  # Adjacency list with weights

    # Heuristic function (just gives a simple guess of distance to goal)
    def heuristic(self, node):
        H = {'A': 1, 'B': 1, 'C': 1, 'D': 1}
        return H.get(node, 0)

    def a_star(self, start, goal):
        open_set = set([start])
        came_from = {}  # For storing path
        g_score = {start: 0}

        while open_set:
            current = min(open_set, key=lambda x: g_score[x] + self.heuristic(x))

            if current == goal:
                # Reconstruct path
                path = [current]
                while current in came_from:
                    current = came_from[current]
                    path.append(current)
                path.reverse()
                print("Path found:", path)
                return path

            open_set.remove(current)

            for neighbor, cost in self.graph.get(current, []):
                temp_g = g_score[current] + cost
                if neighbor not in g_score or temp_g < g_score[neighbor]:
                    g_score[neighbor] = temp_g
                    came_from[neighbor] = current
                    open_set.add(neighbor)

        print("Path not found!")
        return None

# ---------- Example graph ----------
graph_data = {
    'A': [('B', 1), ('C', 3), ('D', 7)],
    'B': [('D', 5)],
    'C': [('D', 12)]
}

g = Graph(graph_data)
g.a_star('A', 'D')

  </pre>

<h2>naive bayes</h2>

  <pre>
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix

# Load data
iris = load_iris()
X = iris.data
y = iris.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)

# Train model
model = GaussianNB()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Accuracy and confusion matrix
print("Accuracy (%):", accuracy_score(y_test, y_pred) * 100)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

  </pre>

<h2>bayesian network</h2>

  <pre>
import bayespy as bp
import numpy as np

# Simplified dataset: [Age, Cholesterol, HeartDisease]
# 0: young, 1: old | 0: normal, 1: high | 0: no, 1: yes
data = np.array([
    [0, 1, 1],
    [1, 1, 1],
    [0, 0, 0],
    [1, 0, 0],
    [1, 1, 1],
    [0, 1, 1]
])
N = len(data)

# Priors
p_age = bp.nodes.Dirichlet(np.ones(2))
age = bp.nodes.Categorical(p_age, plates=(N,))
age.observe(data[:, 0])

p_chol = bp.nodes.Dirichlet(np.ones(2))
chol = bp.nodes.Categorical(p_chol, plates=(N,))
chol.observe(data[:, 1])

# Heart disease depends on age and cholesterol
p_hd = bp.nodes.Dirichlet(np.ones(2), plates=(2, 2))  # (age, chol)
heart = bp.nodes.MultiMixture([age, chol], bp.nodes.Categorical, p_hd)
heart.observe(data[:, 2])

# Inference
p_hd.update()

# Predict for: old (1), high cholesterol (1)
test = bp.nodes.MultiMixture([1, 1], bp.nodes.Categorical, p_hd)
print("Probability of heart disease:", test.get_moments()[0][1])

  </pre>
<h2>regression model</h2>

  <pre>
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
import matplotlib.pyplot as plt

# Create random data
X = np.random.rand(100, 1) * 10  # 100 values from 0 to 10
y = 2 * X + 1 + np.random.randn(100, 1)  # y = 2x + 1 + some noise

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict test data
y_pred = model.predict(X_test)

# Show error
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))

# Plot
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, model.predict(X), color='red', label='Best Fit Line')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title('Simple Linear Regression')
plt.show()

  </pre>

<h2>decision tree</h2>

  <pre>
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.metrics import accuracy_score, confusion_matrix
from matplotlib.colors import ListedColormap
import pydotplus
from six import StringIO
from IPython.display import Image

# Load and prepare data
data = pd.read_csv('Social_Network_Ads.csv')
x = data.iloc[:, [2, 3]].values  # Age and EstimatedSalary
y = data.iloc[:, 4].values       # Purchased

# Split the dataset
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)

# Feature scaling
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

# Train the Decision Tree Classifier
clf = DecisionTreeClassifier(criterion="gini", max_depth=3)
clf.fit(x_train, y_train)

# Predict the test set results
y_pred = clf.predict(x_test)

# Accuracy and confusion matrix
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Visualize decision boundary
x1, x2 = np.meshgrid(
    np.arange(x_test[:, 0].min() - 1, x_test[:, 0].max() + 1, 0.01),
    np.arange(x_test[:, 1].min() - 1, x_test[:, 1].max() + 1, 0.01)
)
plt.contourf(x1, x2, clf.predict(np.c_[x1.ravel(), x2.ravel()]).reshape(x1.shape),
             alpha=0.75, cmap=ListedColormap(["red", "green"]))

for i, j in enumerate(np.unique(y_test)):
    plt.scatter(x_test[y_test == j, 0], x_test[y_test == j, 1],
                c=ListedColormap(["red", "green"])(i), label=j)

plt.title("Decision Tree (Test Set)")
plt.xlabel("Age")
plt.ylabel("Estimated Salary")
plt.legend()
plt.show()

# Export the tree as a .png image
dot = StringIO()
export_graphviz(clf, out_file=dot, filled=True, rounded=True, special_characters=True,
                feature_names=['Age', 'EstimatedSalary'], class_names=['0', '1'])
graph = pydotplus.graph_from_dot_data(dot.getvalue())
graph.write_png('opt_decisiontree_gini.png')
Image(graph.create_png())


  </pre>

<h2>random forest</h2>

  <pre>
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
Y = iris.target  # Target labels (0, 1, 2 for different species)

# Splitting data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)

# Feature Scaling (Not necessary for decision trees but improves performance in some cases)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create a RandomForest classifier instance
clf = RandomForestClassifier(n_estimators=100, random_state=1)
clf.fit(X_train, y_train)  # Train the model

# Predict on test set
y_pred = clf.predict(X_test)

# Evaluate the model
cm = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)

# Print results
print("Confusion Matrix:\n", cm)
print("Accuracy Score:", accuracy)

  </pre>

<h2>svm</h2>

  <pre>
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.metrics import classification_report

# 1. Load dataset
df = pd.read_csv('emails.csv')  # Assumes 'text' and 'spam' columns

# 2. Clean text (optional for basic model)
df['text'] = df['text'].str.replace(r'[^\w\s]', '', regex=True).str.lower()

# 3. Convert text to features
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['text'])

# 4. Set labels
y = df['spam']  # 0 = ham, 1 = spam

# 5. Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 6. Train SVM model
model = SVC(kernel='linear')
model.fit(X_train, y_train)

# 7. Predict and evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

  </pre>

<h2>ensembling</h2>

  <pre>
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import mean_squared_error

# Load dataset
df = pd.read_csv("train_data.csv")
X = df.drop("target", axis=1)
y = df["target"]

# Split data into train (70%), val (20%), test (10%)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3)

# Train base models
lr = LinearRegression().fit(X_train, y_train)
xgb_model = xgb.XGBRegressor().fit(X_train, y_train)
rf = RandomForestRegressor().fit(X_train, y_train)

# Get validation predictions (meta features)
val_preds = pd.DataFrame({
    'lr': lr.predict(X_val),
    'xgb': xgb_model.predict(X_val),
    'rf': rf.predict(X_val)
})

# Get test predictions (for final model prediction)
test_preds = pd.DataFrame({
    'lr': lr.predict(X_test),
    'xgb': xgb_model.predict(X_test),
    'rf': rf.predict(X_test)
})

# Blend using a meta-model (linear regression on predictions)
blender = LinearRegression().fit(val_preds, y_val)
final_preds = blender.predict(test_preds)

# Evaluate
print("Blending MSE:", mean_squared_error(y_test, final_preds))

  </pre> 
  
<h2>clustering</h2>

  <pre>
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import numpy as np
from sklearn import datasets

# Load the Iris dataset
iris = datasets.load_iris()
iris_data = iris.data
iris_labels = iris.target

# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_labels, test_size=0.20, random_state=50)

# Initialize the KNN classifier with n_neighbors=10
classifier = KNeighborsClassifier(n_neighbors=10)

# Train the classifier
classifier.fit(x_train, y_train)

# Make predictions
y_pred = classifier.predict(x_test)

# Print the classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Print the confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

  </pre> 

<h2>em bayesian</h2>

  <pre>
from sklearn.mixture import GaussianMixture
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Load and prepare the Iris dataset
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# Standardize the data (important for EM)
X_scaled = StandardScaler().fit_transform(X)

# Apply EM algorithm using Gaussian Mixture Model
gmm = GaussianMixture(n_components=3)
gmm.fit(X_scaled)
clusters = gmm.predict(X_scaled)

# Visualize clustering results
colors = np.array(['red', 'green', 'blue'])
plt.scatter(X.iloc[:, 2], X.iloc[:, 3], c=colors[clusters], s=40)
plt.title("EM Clustering (GMM)")
plt.xlabel("Petal length")
plt.ylabel("Petal width")
plt.show()

  </pre>

<h2>simple nn</h2>

  <pre>
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the Iris dataset
data = load_iris()

# Create a DataFrame
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

# Split the dataset into features and target
X = df.drop('target', axis=1)
y = df['target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Use a smaller model to avoid perfect accuracy
mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=300, alpha=0.01,
                    solver='adam', random_state=1, tol=0.0001)
mlp.fit(X_train_scaled, y_train)

# Make predictions
y_pred = mlp.predict(X_test_scaled)

# Accuracy and classification report
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, y_pred))


  </pre>

<h2>deep learning nn</h2>

  <pre>
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

# Load CIFAR-10 dataset
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

# Class names for CIFAR-10 dataset
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
                'dog', 'frog', 'horse', 'ship', 'truck']

# Display the first 25 images from the training set
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i])
    plt.xlabel(class_names[train_labels[i][0]])
plt.show()

# Define the CNN model architecture
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10)
])

# Print the model summary
model.summary()

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model
history = model.fit(train_images, train_labels, epochs=10,
                    validation_data=(test_images, test_labels))

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')
plt.show()

# Evaluate the model on test data
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print("\nTest accuracy:", test_acc)


  </pre>  

</body>
</html>
